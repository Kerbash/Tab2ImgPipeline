{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T06:00:31.039317Z",
     "start_time": "2025-06-27T06:00:27.601997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import autokeras as ak\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from typing_extensions import override\n",
    "\n",
    "\"\"\"\n",
    "The datasets folder demands that this structures is followed:\n",
    "datasets/\n",
    "   <FILE_NAME>.csv # where the main tabular data is stored\n",
    "   /<ALGORITHM_NAME_1>/ # where the algorithm specific data is stored\n",
    "       key.csv # where the key for the images are stored\n",
    "       <IMAGE_NAME>.json # the images are stored as an array\n",
    "   /<ALGORITHM_NAME_2>/ # where the algorithm specific data is stored\n",
    "       ...\n",
    "\n",
    "The pipeline will then run the <FILE_NAME>.csv file through a number of traditional models\n",
    "and algorithms. While the converted images are run through a autokeras CNN pipeline.\n",
    "\n",
    "All the results are then saved in the\n",
    "results/<DATASET_NAME>/<EXPERIMENT_NAME>...\n",
    "The following are stored\n",
    "- results.csv # contains the metrics for all the algorithms tested (final results)\n",
    "- <ALGORITHM_NAME>.pkl # contains the trained model for the algorithm\n",
    "\"\"\"\n",
    "DATASETS = [\n",
    "    {\n",
    "        \"path\": \"datasets/sample\",\n",
    "        \"filename\": \"Simple_Classification_Dataset.csv\",\n",
    "        \"target_column\": \"label\"\n",
    "    }\n",
    "]\n",
    "\n",
    "TRADITIONAL_MODELS = {\n",
    "    \"RF\": {\n",
    "        \"class\": RandomForestClassifier,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"class\": SVC,\n",
    "        \"params\": {\n",
    "            \"kernel\": \"rbf\",\n",
    "            \"C\": 1.0,\n",
    "            \"gamma\": 'scale',\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "    },\n",
    "}"
   ],
   "id": "eb6568bbba9241d2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T06:00:31.055281Z",
     "start_time": "2025-06-27T06:00:31.043066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dataloader(path):\n",
    "    \"\"\"\n",
    "    Load the images based on the key.csv file.\n",
    "    Creating a numpy array of images and a list of labels.\n",
    "    :return: (X, y) where X is numpy array of shape (num_samples, height, width, channels)\n",
    "    \"\"\"\n",
    "    # load the key.csv file\n",
    "    df = pd.read_csv(os.path.join(path, \"key.csv\"))\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        filename = row['filename'].replace('.png', '.json')\n",
    "        image_path = os.path.join(path, filename)\n",
    "\n",
    "        # Load the JSON data\n",
    "        with open(image_path, 'r') as f:\n",
    "            image_data = json.load(f)\n",
    "\n",
    "        # Convert to proper numpy array\n",
    "        if isinstance(image_data, list):\n",
    "            # If it's a flat list of RGB values, reshape appropriately\n",
    "            image_array = np.array(image_data)\n",
    "\n",
    "            # Reshape to proper image dimensions (you'll need to know your image dimensions)\n",
    "            # For example, if it's a 28x28 RGB image:\n",
    "            # image_array = image_array.reshape(28, 28, 3)\n",
    "\n",
    "        else:\n",
    "            # If it's already structured, convert directly\n",
    "            image_array = np.array(image_data)\n",
    "\n",
    "        # Ensure proper data type and range\n",
    "        if image_array.max() > 1.0:\n",
    "            image_array = image_array.astype(np.float32) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        x.append(image_array)\n",
    "        y.append(row['label'])\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(x)  # Shape should be (num_samples, height, width, channels)\n",
    "    y = np.array(y)  # Shape should be (num_samples,)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def autokeras_runner(path, n=1, epochs=10, batch_size=None, validation_split=None,\n",
    "                     validation_data=None, max_trials=1, directory='auto_keras',\n",
    "                     algorithm_name='image_classifier', overwrite=True, seed=None,\n",
    "                     max_model_size=None, tuner='random', **kwargs):\n",
    "    \"\"\"\n",
    "    Load the dataset from the given path and run AutoKeras with configurable parameters.\n",
    "    \"\"\"\n",
    "    # get the data\n",
    "    x, y = dataloader(path)\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    # Set default batch_size if not provided\n",
    "    if batch_size is None:\n",
    "        batch_size = min(32, len(x) // 2) if len(x) < 64 else 32\n",
    "\n",
    "    # Set default validation if not provided\n",
    "    if validation_split is None and validation_data is None:\n",
    "        if len(x) < 64:\n",
    "            validation_data = (x, y)\n",
    "        else:\n",
    "            validation_split = 0.2\n",
    "\n",
    "    for run in range(n):\n",
    "        start_time = time.time()\n",
    "\n",
    "        clf = ak.ImageClassifier(\n",
    "            max_trials=max_trials,\n",
    "            directory=directory,\n",
    "            project_name=f\"{algorithm_name}_run_{run}\",\n",
    "            overwrite=overwrite,\n",
    "            seed=seed,\n",
    "            max_model_size=max_model_size,\n",
    "            tuner=tuner\n",
    "        )\n",
    "\n",
    "        fit_kwargs = {\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            **kwargs\n",
    "        }\n",
    "\n",
    "        if validation_data is not None:\n",
    "            fit_kwargs['validation_data'] = validation_data\n",
    "        elif validation_split is not None:\n",
    "            fit_kwargs['validation_split'] = validation_split\n",
    "\n",
    "        clf.fit(x, y, **fit_kwargs)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        model = clf.export_model()\n",
    "        y_pred = model.predict(x)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # calculate metrics\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        precision = precision_score(y, y_pred, average='weighted')\n",
    "        recall = recall_score(y, y_pred, average='weighted')\n",
    "        f1 = f1_score(y, y_pred, average='weighted')\n",
    "\n",
    "        results_list.append({\n",
    "            'model_name': 'autokeras_cnn',\n",
    "            'run': run + 1,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'train_time': train_time\n",
    "        })\n",
    "\n",
    "    return results_list\n",
    "\n",
    "\n",
    "def get_algorithm_dir(path):\n",
    "    algorithm_dirs = [d for d in os.listdir(dataset[\"path\"]) if os.path.isdir(os.path.join(dataset[\"path\"], d))]\n",
    "    # for each of those algorithm dir check if there is a key.csv file if not remove it from the list\n",
    "    algorithm_dirs = [d for d in algorithm_dirs if os.path.isfile(os.path.join(path, d, \"key.csv\"))]\n",
    "\n",
    "    return algorithm_dirs\n",
    "\n",
    "\n",
    "def autokeras_cnn_pipeline(path, output_folder, all_result_df, all_result_df_path=None):\n",
    "    \"\"\"\n",
    "    Run the autokeras CNN pipeline on the dataset.\n",
    "    The dataset should be a CSV file with a target column.\n",
    "    \"\"\"\n",
    "    # get the list of all algorithm in the dataset folder\n",
    "    algorithms = get_algorithm_dir(path)\n",
    "\n",
    "    # for each algorithm run the autokeras CNN pipeline\n",
    "    for algorithm in algorithms:\n",
    "        # get the path to the algorithm folder\n",
    "        algorithm_path = os.path.join(path, algorithm)\n",
    "\n",
    "        # check if the key.csv file exists\n",
    "        result = autokeras_runner(algorithm_path, directory=output_folder, algorithm_name=algorithm)\n",
    "        # convert the results to a DataFrame\n",
    "        result_df = pd.DataFrame(result)\n",
    "        # merge the results with the main DataFrame\n",
    "        all_result_df = pd.concat([all_result_df, result_df], ignore_index=True)\n",
    "        # save the result df to a CSV file at interval to make sure we don't lose results\n",
    "        all_result_df.to_csv(all_result_df_path, index=False, overwrite=True)\n",
    "\n",
    "    # convert the results to a DataFrame\n",
    "    return all_result_df.copy()"
   ],
   "id": "a111887b15db9f13",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from libs.traditional_classification_pipeline import tabular_testing\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    # for a given dataset create a new experiment folder\n",
    "    output_folder = f\"results/{dataset['path'].split('/')[-1]}/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # try to run the traditional models first\n",
    "    results = tabular_testing(file_name=os.path.join(dataset[\"path\"], dataset[\"filename\"]), target_column=dataset[\"target_column\"], model_configs=TRADITIONAL_MODELS)\n",
    "    results.to_csv(f\"{output_folder}/results.csv\", index=False, overwrite=True)\n",
    "    # get all the \"directories\" in the dataset folder\n",
    "    results = autokeras_cnn_pipeline(dataset[\"path\"], output_folder=output_folder, all_result_df=results, all_result_df_path=f\"{output_folder}/results.csv\")\n",
    "\n",
    "    # save the results\n",
    "    #results = tab_result\n",
    "    results.to_csv(f\"{output_folder}/results.csv\", index=False, overwrite=True)\n",
    "print(\"DONE!!!\")"
   ],
   "id": "a6f8fe5285fa5cb",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 42s]\n",
      "val_loss: 0.6931465268135071\n",
      "\n",
      "Best val_loss So Far: 0.6931465268135071\n",
      "Total elapsed time: 00h 00m 42s\n",
      "Epoch 1/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 1s/step - accuracy: 0.2667 - loss: 0.7487 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 2/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260ms/step - accuracy: 0.4667 - loss: 0.6968 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 267ms/step - accuracy: 0.6000 - loss: 0.6693 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 4/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 278ms/step - accuracy: 0.9333 - loss: 0.5662 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 280ms/step - accuracy: 0.5333 - loss: 0.6793 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 6/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 258ms/step - accuracy: 0.6000 - loss: 0.6794 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 7/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 270ms/step - accuracy: 0.6000 - loss: 0.6234 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 8/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 246ms/step - accuracy: 0.8000 - loss: 0.5502 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 254ms/step - accuracy: 0.8667 - loss: 0.5338 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 10/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259ms/step - accuracy: 0.8000 - loss: 0.5883 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 772ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python Project\\Tab2Img\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "True              |True              |image_block_1/normalize\n",
      "False             |False             |image_block_1/augment\n",
      "xception          |xception          |image_block_1/block_type\n",
      "global_avg        |global_avg        |classification_head_1/spatial_reduction_1/reduction_type\n",
      "0.25              |0.25              |classification_head_1/dropout\n",
      "sgd               |sgd               |optimizer\n",
      "0.0001            |0.0001            |learning_rate\n",
      "\n",
      "Epoch 1/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 1s/step - accuracy: 0.6000 - loss: 0.6718 - val_accuracy: 0.6000 - val_loss: 0.6931\n",
      "Epoch 2/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 6s/step - accuracy: 0.4667 - loss: 0.6761 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 263ms/step - accuracy: 0.6000 - loss: 0.6527 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 4/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 4s/step - accuracy: 0.6000 - loss: 0.6465 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 257ms/step - accuracy: 0.6000 - loss: 0.6356 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 6/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 7s/step - accuracy: 0.4667 - loss: 0.6675 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 7/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 338ms/step - accuracy: 0.6000 - loss: 0.6400 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 8/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 295ms/step - accuracy: 0.6000 - loss: 0.6593 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 150ms/step - accuracy: 0.6500 - loss: 0.6060"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e77ace645858780d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
